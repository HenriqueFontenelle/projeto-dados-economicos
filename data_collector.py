# data_collector.py - Vers√£o melhorada para coleta robusta de dados
import requests
import pandas as pd
import json
from datetime import datetime, timedelta
import time
import logging
from typing import Dict, List, Optional, Tuple
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import config, get_date_range

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BCBDataCollector:
    """
    Coletor de dados robusto do Banco Central do Brasil
    
    Melhorias implementadas:
    - Coleta de 10 anos de dados por padr√£o
    - Tratamento robusto de erros
    - Coleta paralela para melhor performance
    - Valida√ß√£o de dados
    - Retry autom√°tico em caso de falhas
    - Logging detalhado
    """
    
    def __init__(self):
        self.base_url = config.data_collection.bcb_base_url
        self.indicators = config.data_collection.indicators
        self.max_retries = config.data_collection.max_retries
        self.timeout = config.data_collection.request_timeout
        self.delay = config.data_collection.delay_between_requests
        
        # Estat√≠sticas de coleta
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_records': 0
        }
    
    def _make_request(self, url: str, retries: int = None) -> Optional[List[Dict]]:
        """
        Faz requisi√ß√£o HTTP com retry autom√°tico
        
        Args:
            url: URL para requisi√ß√£o
            retries: N√∫mero de tentativas (opcional)
        
        Returns:
            Dados JSON ou None em caso de erro
        """
        if retries is None:
            retries = self.max_retries
        
        self.stats['total_requests'] += 1
        
        for attempt in range(retries + 1):
            try:
                logger.debug(f"Tentativa {attempt + 1} para URL: {url}")
                
                response = requests.get(url, timeout=self.timeout)
                response.raise_for_status()
                
                data = response.json()
                
                if data:
                    self.stats['successful_requests'] += 1
                    return data
                else:
                    logger.warning(f"Resposta vazia para URL: {url}")
                    
            except requests.exceptions.RequestException as e:
                logger.warning(f"Erro na tentativa {attempt + 1}: {e}")
                if attempt < retries:
                    time.sleep(self.delay * (attempt + 1))  # Backoff exponencial
                else:
                    logger.error(f"Falha ap√≥s {retries + 1} tentativas para URL: {url}")
                    self.stats['failed_requests'] += 1
            
            except json.JSONDecodeError as e:
                logger.error(f"Erro ao decodificar JSON: {e}")
                self.stats['failed_requests'] += 1
                break
        
        return None
    
    def _validate_data(self, df: pd.DataFrame, indicator: str) -> bool:
        """
        Valida os dados coletados
        
        Args:
            df: DataFrame com os dados
            indicator: Nome do indicador
        
        Returns:
            True se dados s√£o v√°lidos
        """
        if df is None or df.empty:
            logger.warning(f"DataFrame vazio para {indicator}")
            return False
        
        required_columns = ['date', 'value']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            logger.error(f"Colunas obrigat√≥rias ausentes para {indicator}: {missing_columns}")
            return False
        
        # Verificar se h√° valores v√°lidos
        valid_values = df['value'].notna().sum()
        if valid_values == 0:
            logger.warning(f"Nenhum valor v√°lido encontrado para {indicator}")
            return False
        
        # Verificar formato de datas
        try:
            pd.to_datetime(df['date'])
        except Exception as e:
            logger.error(f"Erro no formato de datas para {indicator}: {e}")
            return False
        
        logger.info(f"Dados v√°lidos para {indicator}: {len(df)} registros, {valid_values} valores v√°lidos")
        return True
    
    def get_data(self, indicator: str, start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """
        Obt√©m dados de um indicador espec√≠fico
        
        Args:
            indicator: Nome do indicador (chave do dicion√°rio self.indicators)
            start_date: Data inicial (formato 'DD/MM/AAAA', opcional)
            end_date: Data final (formato 'DD/MM/AAAA', opcional)
        
        Returns:
            DataFrame com os dados do indicador ou None
        """
        if indicator not in self.indicators:
            logger.error(f"Indicador '{indicator}' n√£o reconhecido")
            return None
        
        # Usar configura√ß√£o padr√£o se datas n√£o fornecidas
        if start_date is None or end_date is None:
            start_date, end_date = get_date_range()
        
        # Construir URL
        serie_id = self.indicators[indicator]['series_id']
        url = f"{self.base_url}.{serie_id}/dados?formato=json"
        url += f"&dataInicial={start_date}&dataFinal={end_date}"
        
        logger.info(f"Coletando dados para {indicator} (s√©rie {serie_id}) de {start_date} a {end_date}")
        
        # Fazer requisi√ß√£o
        data = self._make_request(url)
        
        if not data:
            logger.error(f"Falha ao obter dados para {indicator}")
            return None
        
        try:
            # Converter para DataFrame
            df = pd.DataFrame(data)
            
            # Renomear colunas para padr√£o
            df.rename(columns={
                'data': 'date',
                'valor': 'value'
            }, inplace=True)
            
            # Converter tipos de dados
            df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')
            df['value'] = pd.to_numeric(df['value'], errors='coerce')
            
            # Adicionar metadados
            df['indicator'] = indicator
            df['series_id'] = serie_id
            df['collected_at'] = datetime.now()
            
            # Ordenar por data
            df = df.sort_values('date').reset_index(drop=True)
            
            # Validar dados
            if self._validate_data(df, indicator):
                self.stats['total_records'] += len(df)
                logger.info(f"Coletados {len(df)} registros para {indicator}")
                return df
            else:
                return None
                
        except Exception as e:
            logger.error(f"Erro ao processar dados para {indicator}: {e}")
            return None
    
    def collect_indicator_batch(self, indicators: List[str], start_date: str = None, end_date: str = None) -> Dict[str, pd.DataFrame]:
        """
        Coleta dados para m√∫ltiplos indicadores em paralelo
        
        Args:
            indicators: Lista de indicadores
            start_date: Data inicial
            end_date: Data final
        
        Returns:
            Dict com DataFrames para cada indicador
        """
        results = {}
        
        def collect_single(indicator):
            try:
                return indicator, self.get_data(indicator, start_date, end_date)
            except Exception as e:
                logger.error(f"Erro ao coletar {indicator}: {e}")
                return indicator, None
        
        # Usar ThreadPoolExecutor para coleta paralela
        with ThreadPoolExecutor(max_workers=3) as executor:  # Limitado para n√£o sobrecarregar a API
            future_to_indicator = {
                executor.submit(collect_single, indicator): indicator 
                for indicator in indicators
            }
            
            for future in as_completed(future_to_indicator):
                indicator, df = future.result()
                if df is not None:
                    results[indicator] = df
                    logger.info(f"‚úì Conclu√≠do: {indicator}")
                else:
                    logger.warning(f"‚úó Falhou: {indicator}")
                
                # Pequeno delay entre conclus√µes
                time.sleep(self.delay)
        
        return results
    
    def collect_all_data(self, last_n_years: int = None, indicators: List[str] = None) -> Dict[str, pd.DataFrame]:
        """
        Coleta dados de todos os indicadores ou lista espec√≠fica
        
        Args:
            last_n_years: N√∫mero de anos retroativos (padr√£o: configura√ß√£o)
            indicators: Lista espec√≠fica de indicadores (opcional)
        
        Returns:
            Dict com DataFrames para cada indicador
        """
        # Usar configura√ß√£o padr√£o se n√£o especificado
        if last_n_years is None:
            last_n_years = config.data_collection.default_years
        
        # Usar todos os indicadores se n√£o especificado
        if indicators is None:
            indicators = list(self.indicators.keys())
        
        # Calcular datas
        start_date, end_date = get_date_range(last_n_years)
        
        logger.info(f"Iniciando coleta de {len(indicators)} indicadores para {last_n_years} anos")
        logger.info(f"Per√≠odo: {start_date} a {end_date}")
        
        # Resetar estat√≠sticas
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_records': 0
        }
        
        start_time = time.time()
        
        # Coletar dados
        results = self.collect_indicator_batch(indicators, start_date, end_date)
        
        # Estat√≠sticas finais
        duration = time.time() - start_time
        success_rate = (self.stats['successful_requests'] / max(self.stats['total_requests'], 1)) * 100
        
        logger.info(f"Coleta conclu√≠da em {duration:.2f}s")
        logger.info(f"Indicadores coletados: {len(results)}/{len(indicators)}")
        logger.info(f"Taxa de sucesso: {success_rate:.1f}%")
        logger.info(f"Total de registros: {self.stats['total_records']}")
        
        return results
    
    def get_collection_stats(self) -> Dict:
        """
        Retorna estat√≠sticas da √∫ltima coleta
        
        Returns:
            Dict com estat√≠sticas
        """
        return self.stats.copy()
    
    def check_api_status(self) -> bool:
        """
        Verifica se a API do BCB est√° respondendo
        
        Returns:
            True se API est√° OK
        """
        test_url = f"{self.base_url}.433/dados?formato=json&dataInicial=01/01/2024&dataFinal=01/01/2024"
        
        try:
            response = requests.get(test_url, timeout=10)
            response.raise_for_status()
            logger.info("API do BCB est√° respondendo normalmente")
            return True
        except Exception as e:
            logger.error(f"API do BCB n√£o est√° respondendo: {e}")
            return False
    
    def get_available_periods(self, indicator: str) -> Optional[Tuple[datetime, datetime]]:
        """
        Obt√©m o per√≠odo dispon√≠vel para um indicador
        
        Args:
            indicator: Nome do indicador
        
        Returns:
            Tuple (data_inicial, data_final) ou None
        """
        if indicator not in self.indicators:
            return None
        
        # Fazer requisi√ß√£o sem filtro de data para ver per√≠odo completo
        serie_id = self.indicators[indicator]['series_id']
        url = f"{self.base_url}.{serie_id}/dados?formato=json"
        
        data = self._make_request(url)
        
        if data:
            try:
                df = pd.DataFrame(data)
                df['data'] = pd.to_datetime(df['data'], format='%d/%m/%Y')
                return df['data'].min(), df['data'].max()
            except Exception as e:
                logger.error(f"Erro ao obter per√≠odo para {indicator}: {e}")
        
        return None

# Fun√ß√£o de conveni√™ncia para uso em outros m√≥dulos
def quick_collect(indicators: List[str] = None, years: int = None) -> Dict[str, pd.DataFrame]:
    """
    Fun√ß√£o de conveni√™ncia para coleta r√°pida de dados
    
    Args:
        indicators: Lista de indicadores (opcional, usa todos se n√£o especificado)
        years: N√∫mero de anos (opcional, usa configura√ß√£o padr√£o)
    
    Returns:
        Dict com DataFrames dos indicadores
    """
    collector = BCBDataCollector()
    return collector.collect_all_data(years, indicators)

if __name__ == "__main__":
    # Teste da classe melhorada
    collector = BCBDataCollector()
    
    # Verificar status da API
    if not collector.check_api_status():
        print("‚ö†Ô∏è API do BCB n√£o est√° respondendo. Tentando mesmo assim...")
    
    # Teste com alguns indicadores principais por 2 anos (para n√£o demorar muito)
    test_indicators = ['ipca', 'selic', 'pib']
    print(f"\nüìä Testando coleta de {test_indicators} por 2 anos...")
    
    data = collector.collect_all_data(last_n_years=2, indicators=test_indicators)
    
    # Exibir resultados
    print(f"\nüìà Resultados da coleta:")
    for indicator, df in data.items():
        if df is not None:
            info = collector.indicators[indicator]
            print(f"\n{info['name']} ({indicator}):")
            print(f"  ‚Ä¢ Per√≠odo: {df['date'].min().strftime('%d/%m/%Y')} a {df['date'].max().strftime('%d/%m/%Y')}")
            print(f"  ‚Ä¢ Registros: {len(df)}")
            print(f"  ‚Ä¢ Valores v√°lidos: {df['value'].notna().sum()}")
            print(f"  ‚Ä¢ √öltimo valor: {df['value'].iloc[-1]:.4f} {info.get('unit', '')}")
    
    # Exibir estat√≠sticas
    stats = collector.get_collection_stats()
    print(f"\nüìä Estat√≠sticas da coleta:")
    print(f"  ‚Ä¢ Total de requisi√ß√µes: {stats['total_requests']}")
    print(f"  ‚Ä¢ Requisi√ß√µes bem-sucedidas: {stats['successful_requests']}")
    print(f"  ‚Ä¢ Requisi√ß√µes com falha: {stats['failed_requests']}")
    print(f"  ‚Ä¢ Total de registros coletados: {stats['total_records']}")